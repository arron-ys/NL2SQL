# NL2SQL Service 环境变量配置

# ============================================================
# 日志配置 (Logging Configuration)
# ============================================================
# 日志级别: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO


# ============================================================
# 流水线配置 (Pipeline Configuration)
# 用途：日常模型提交、服务调优、TUNING
# 说明： 以下配置项均有默认值；如需覆盖，取消注释并填写新值
#============================================================

# ================= Stage 2: 检索与生成 (Retrieval & Generation) =================

# [调参锚点 1] 向量检索召回数量 ：向量数据库初步检索出的候选 Term 数量（Qdrant limit）
# 默认：30；建议：20-30 ；过少易漏关键指标；过多会增加后续处理开销与噪音
# VECTOR_SEARCH_TOP_K=30

# [调参锚点 2] 最终召回上限 ：混合检索(A精确 + B向量)合并去重后，最终喂给 LLM 的 Term 数量上限
# 默认：20；建议：20-50 ；过小会漏关键信息；过大可能导致 LLM 注意力分散与 Token 浪费
# MAX_TERM_RECALL=20

# [调参锚点 3] 相似度阈值 ：向量检索分数的最低门槛；低于该门槛的候选直接丢弃（宁缺毋滥）
# 默认：0.4；建议：0.4-0.6 ；需结合 Embedding 模型效果调优
# SIMILARITY_THRESHOLD=0.4

# [调参锚点 4] 描述最大长度 ：Schema Context 中单个 Term 的 description 字符截断长度（中文字符）
# 默认：50；建议：50-100 ；防止单个字段长描述挤占 Token
# MAX_DESCRIPTION_LENGTH=50

# [调参锚点 5] 枚举展示上限 ：Schema Context 中枚举值展示策略（<=阈值完整展示；>阈值不展示，避免 Token 爆炸与误导）
# 默认：50；建议：20-50 ；若枚举值很长或噪音大可调低
# MAX_ENUM_VALUES_DISPLAY=50


# ================= Stage 3: 校验与补全 (Validation & Normalization) =================

# [调参锚点 6] 默认 Limit ：当用户未指定 Top N / limit 时，Plan 自动填充的 limit 值
# 默认：100；建议：100 ；可按业务场景调整（但不建议过大）
# DEFAULT_LIMIT=100

# [调参锚点 7] 最大 Limit 上限 ：即使用户指定 limit，也不允许超过此值（防止误操作/恶意拉取过大结果集）
# 默认：1000；建议：1000-5000 ；需与 MAX_RESULT_ROWS 协调（MAX_RESULT_ROWS 应 >= MAX_LIMIT_CAP）
# MAX_LIMIT_CAP=1000


# ================= Stage 4: SQL 生成 (SQL Generation) =================

# [调参锚点 8] 生成的SQL类型 ：SQL 生成与执行的目标数据库方言（影响 SQL 语法差异）
# 默认：mysql；可选：mysql, postgresql ；需与实际数据库类型一致
# DB_TYPE=mysql


# ================= Stage 5: 执行 (Execution) =================

# [调参锚点 10] SQL 执行超时(ms) ：数据库会话强制超时（毫秒）
# 默认：5000；建议：5000-10000 ；服务型 API 不宜过长，避免连接被长查询占满
# EXECUTION_TIMEOUT_MS=5000

# [调参锚点 11] 结果集硬截断行数 ：后端从数据库 fetch 的最大物理行数，防止 OOM（应 >= MAX_LIMIT_CAP）
# 默认：5000；建议：5000 ；若 MAX_LIMIT_CAP 调大，这里也要同步调大
# MAX_RESULT_ROWS=5000


# ================= Stage 6: 答案生成 (Answer) =================

# [调参锚点 12] LLM 最大行数 ：喂给 LLM 的最大结果行数（防止 Prompt 过长）
# 默认：50；建议：20-100 ；行数越大 Token 越高，且回答质量可能下降
# MAX_LLM_ROWS=50



# ============================================================
# LLM 通用配置 (LLM Configuration)
# ============================================================

# 默认LLM模型：优先用下方显式设置的 DEFAULT_LLM_PROVIDER
# 如果没配置（或配置无效），才走自动选择，现在的优先级是：DeepSeek > Qwen > OpenAI，但前提是对应的 API_KEY 存在
DEFAULT_LLM_PROVIDER=Qwen


# 通用超时配置 (General Timeout Configuration)
# 通用 LLM 超时配置（所有 LLM provider 都使用，除非设置了 provider 特定超时）
# 默认值：60.0 秒（OpenAI/DeepSeek/Qwen），30.0 秒（Jina）
LLM_TIMEOUT=60.0


# ============================================================
# OpenAI 配置 (OpenAI Configuration)
# ============================================================
# OpenAI API Key (必需)
OPENAI_API_KEY=

# OpenAI Base URL (可选，用于自定义 API 端点，如使用代理)
# 如果不设置，将使用 OpenAI 官方 API
OPENAI_BASE_URL=

# OpenAI 模型配置 (可选，不设置则使用默认值)
# 查询分解模型 (默认: gpt-4o-mini)
# 可选值: gpt-4o-mini, gpt-4o, gpt-4-turbo, gpt-3.5-turbo 等
OPENAI_MODEL_QUERY_DECOMPOSITION=gpt-4o-mini

# 计划生成模型 (默认: gpt-4o-mini)
# 可选值: gpt-4o-mini, gpt-4o, gpt-4-turbo, gpt-3.5-turbo 等
OPENAI_MODEL_PLAN_GENERATION=gpt-4o-mini

# 答案生成模型 (默认: gpt-4o-mini)
# 可选值: gpt-4o-mini, gpt-4o, gpt-4-turbo, gpt-3.5-turbo 等
OPENAI_MODEL_ANSWER_GENERATION=gpt-4o-mini

# OpenAI 超时配置 (可选，默认: 60 秒)
# OPENAI_TIMEOUT=60.0  # 可选，如果设置了会覆盖 LLM_TIMEOUT



# ============================================================
# DeepSeek 配置 (DeepSeek Configuration)
# ============================================================
# DeepSeek API Key (可选，如果使用 DeepSeek 模型)
# DeepSeek 使用 OpenAI 兼容的 API 接口
DEEPSEEK_API_KEY=

# DeepSeek Base URL (可选，默认值为 https://api.deepseek.com)
DEEPSEEK_BASE_URL=https://api.deepseek.com

# DeepSeek 模型配置 (可选，不设置则使用默认值)
# 查询分解模型 (默认: deepseek-chat)
DEEPSEEK_MODEL_QUERY_DECOMPOSITION=deepseek-chat

# 计划生成模型 (默认: deepseek-reasoner，推理模型)
DEEPSEEK_MODEL_PLAN_GENERATION=deepseek-reasoner

# 答案生成模型 (默认: deepseek-chat)
DEEPSEEK_MODEL_ANSWER_GENERATION=deepseek-chat

# DeepSeek 超时配置 (可选，如果设置了会覆盖 LLM_TIMEOUT)
# DEEPSEEK_TIMEOUT=60.0

# ============================================================
# Qwen 配置 (Qwen Configuration)
# ============================================================
# Qwen API Key (可选，如果使用 Qwen 模型)
# Qwen 使用 OpenAI 兼容的 API 接口（通过阿里云 DashScope）
QWEN_API_KEY=

# Qwen Base URL (可选，默认值为 https://dashscope.aliyuncs.com/compatible-mode/v1)
QWEN_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1

# Qwen 模型配置 (可选，不设置则使用默认值)
# 查询分解模型 (默认: qwen-turbo，快速)
QWEN_MODEL_QUERY_DECOMPOSITION=qwen-turbo

# 计划生成模型 (默认: qwen-max，高质量)
QWEN_MODEL_PLAN_GENERATION=qwen-max

# 答案生成模型 (默认: qwen-plus，平衡)
QWEN_MODEL_ANSWER_GENERATION=qwen-plus

# Qwen 超时配置 (可选，如果设置了会覆盖 LLM_TIMEOUT)
# QWEN_TIMEOUT=60.0

# ============================================================
# Jina 配置 (Jina Configuration)
# ============================================================
# Jina AI Embedding API Key (用于生成文本嵌入向量)
# 如果不设置，向量搜索功能将不可用
JINA_API_KEY=

# Jina Base URL (可选，Jina 有默认的 base_url)
JINA_BASE_URL=

# Jina 超时配置 (可选，如果设置了会覆盖 LLM_TIMEOUT)
# 默认值：30.0 秒
JINA_TIMEOUT=30.0

# Jina 模型配置 (可选，不设置则使用默认值)
# 嵌入模型 (默认: jina-embeddings-v3)
# 可选值: jina-embeddings-v3, jina-embeddings-v2 等
JINA_MODEL_EMBEDDING=jina-embeddings-v3

# ============================================================
# 向量数据库配置 (Vector Database Configuration)
# ============================================================

# 向量存储模式：local（本地文件系统，默认）/ memory（内存）/ remote（远程服务）
# 默认值：local
VECTOR_STORE_MODE=local

# 本地存储路径（仅 local 模式有效）
# 如果未设置，默认使用项目根目录下的 qdrant_data/ 文件夹
# 示例：VECTOR_STORE_PATH=../qdrant_data
# VECTOR_STORE_PATH=

# Qdrant 远程服务配置（仅 remote 模式有效）
# 方式1：使用 URL（推荐）
# QDRANT_URL=http://localhost:6333

# 方式2：使用 Host + Port（向后兼容）
QDRANT_HOST=localhost
QDRANT_PORT=6333
# QDRANT_API_KEY=  # 可选，如果 Qdrant 需要认证

# ============================================================
# 语义层配置 (Semantics Configuration)
# ============================================================
# YAML 配置文件目录路径（相对于项目根目录）
# 默认值为 "semantics"
SEMANTICS_YAML_PATH=semantics


# ============================================================
# 代理配置 (Proxy Configuration)
# ============================================================
# 说明：
# - 默认不信任系统代理环境变量（HTTP_PROXY/HTTPS_PROXY/ALL_PROXY），除非显式设置 PROXY_MODE=system
# - OpenAI / DeepSeek / Qwen / Jina 分别使用各自的 *_PROXY，避免互相污染
#
# PROXY_MODE:
# - none:    禁用所有代理（强制直连），忽略系统代理 env（trust_env=False）
# - explicit:（默认/推荐）仅使用 provider 专用代理（*_PROXY），忽略系统代理 env（trust_env=False）
# - system:  信任系统代理 env（HTTP_PROXY/HTTPS_PROXY/ALL_PROXY），即 trust_env=True
PROXY_MODE=explicit
#
# PROXY_STRICT:
# - 0（默认）: 显式代理不可达则自动降级直连，并强制 trust_env=False（避免再次被系统代理 env 劫持）
# - 1        : 显式代理不可达则直接报错（用于快速定位端口/进程问题）
PROXY_STRICT=0
#
# Provider 专用代理（仅在 PROXY_MODE=explicit 时使用）
OPENAI_PROXY=http://127.0.0.1:7897
DEEPSEEK_PROXY=
QWEN_PROXY=
# Jina 默认也不读取系统 HTTP_PROXY/HTTPS_PROXY；仅在 PROXY_MODE=system 时才信任系统 env
JINA_PROXY=
#
# 系统级代理（仅在 PROXY_MODE=system 时才会生效）
HTTP_PROXY=
HTTPS_PROXY=

# ============================================================
# 数据库配置 (Database Configuration)
# ============================================================
# 数据库类型: mysql 或 postgresql
DB_TYPE=mysql

# MySQL/PostgreSQL 连接配置
DB_HOST=localhost
DB_PORT=3306
DB_USER=root
DB_PASSWORD=
DB_NAME=

 
