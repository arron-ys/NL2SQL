# ============================================================
# NL2SQL Service 环境变量配置
# ============================================================
# 请根据实际情况填写以下配置项
# 注意：此文件包含敏感信息，请勿提交到版本控制系统

# ============================================================

# ============================================================
# 快速配置示例：切换到 DeepSeek
# ============================================================
# 如果你想要使用 DeepSeek 作为默认 LLM Provider，请按以下步骤配置：
#
# 1. 明确指定使用 DeepSeek（取消下面的注释）
# DEFAULT_LLM_PROVIDER=deepseek
#
# 2. 在下面的 "DeepSeek 配置" 部分填入你的 DEEPSEEK_API_KEY
#
# 3. （可选）配置 DeepSeek 的模型和超时设置
#
# 完整的 DeepSeek 配置示例请参考下面的 "DeepSeek 配置" 部分
# ============================================================

# 数据库配置 (Database Configuration)
# ============================================================
# 数据库类型: mysql 或 postgresql
DB_TYPE=mysql

# MySQL/PostgreSQL 连接配置
DB_HOST=localhost
DB_PORT=3306
DB_USER=root
DB_PASSWORD=
DB_NAME=

# ============================================================
# LLM 配置 (LLM Configuration)

# ============================================================
# 通用超时配置 (General Timeout Configuration)
# ============================================================
# 通用 LLM 超时配置（所有 LLM provider 都使用，除非设置了 provider 特定超时）
# 默认值：60.0 秒（OpenAI/DeepSeek/Qwen），30.0 秒（Jina）
LLM_TIMEOUT=60.0

# 默认 LLM Provider 配置 (可选)
# 可选值: openai, deepseek, qwen
# 如果不设置，将按以下优先级自动选择: DeepSeek > Qwen > OpenAI
# DEFAULT_LLM_PROVIDER=deepseek
# 示例：明确指定使用 DeepSeek
# DEFAULT_LLM_PROVIDER=deepseek
# 示例：明确指定使用 OpenAI
# DEFAULT_LLM_PROVIDER=openai
# 示例：明确指定使用 Qwen
# DEFAULT_LLM_PROVIDER=qwen



# ============================================================
# 通用超时配置 (General Timeout Configuration)
# ============================================================
# 通用 LLM 超时配置（所有 LLM provider 都使用，除非设置了 provider 特定超时）
# 默认值：60.0 秒（OpenAI/DeepSeek/Qwen），30.0 秒（Jina）
LLM_TIMEOUT=60.0

# ============================================================
# OpenAI API Key (必需)
OPENAI_API_KEY=

# OpenAI Base URL (可选，用于自定义 API 端点，如使用代理)
# 如果不设置，将使用 OpenAI 官方 API
OPENAI_BASE_URL=

# OpenAI 模型配置 (可选，不设置则使用默认值)
# 查询分解模型 (默认: gpt-4o-mini)
# 可选值: gpt-4o-mini, gpt-4o, gpt-4-turbo, gpt-3.5-turbo 等
OPENAI_MODEL_QUERY_DECOMPOSITION=gpt-4o-mini

# 计划生成模型 (默认: gpt-4o-mini)
# 可选值: gpt-4o-mini, gpt-4o, gpt-4-turbo, gpt-3.5-turbo 等
OPENAI_MODEL_PLAN_GENERATION=gpt-4o-mini

# 答案生成模型 (默认: gpt-4o-mini)
# 可选值: gpt-4o-mini, gpt-4o, gpt-4-turbo, gpt-3.5-turbo 等
OPENAI_MODEL_ANSWER_GENERATION=gpt-4o-mini

# OpenAI 超时配置 (可选，默认: 60 秒)
# OPENAI_TIMEOUT=60.0  # 可选，如果设置了会覆盖 LLM_TIMEOUT

# ============================================================
# 代理配置 (Proxy Configuration)
# ============================================================
# OpenAI 代理配置 (推荐，专门用于 OpenAI API)
# 如果在中国大陆访问 OpenAI，需要配置代理
# 示例: http://127.0.0.1:7897 (Clash Verge 默认端口)
# 如果不设置，将尝试使用系统级代理 (HTTP_PROXY/HTTPS_PROXY)
# 如果都不设置，则不使用代理（直接连接，可能失败）
OPENAI_PROXY=http://127.0.0.1:7897

# 系统级代理配置 (可选，向后兼容)
# 如果设置了 OPENAI_PROXY，则优先使用 OPENAI_PROXY
# 如果未设置 OPENAI_PROXY，则使用以下系统级代理配置
HTTP_PROXY=http://127.0.0.1:7897
HTTPS_PROXY=http://127.0.0.1:7897

# ============================================================
# DeepSeek 配置 (DeepSeek Configuration)
# ============================================================
# DeepSeek API Key (可选，如果使用 DeepSeek 模型)
# DeepSeek 使用 OpenAI 兼容的 API 接口

# ============================================================
# 切换到 DeepSeek 的完整配置示例
# ============================================================
# 1. 明确指定使用 DeepSeek 作为默认 LLM Provider
DEFAULT_LLM_PROVIDER=deepseek

# 2. 配置 DeepSeek API Key（必需）
DEEPSEEK_API_KEY=

# DeepSeek Base URL (可选，默认值为 https://api.deepseek.com)
DEEPSEEK_BASE_URL=https://api.deepseek.com

# DeepSeek 模型配置 (可选，不设置则使用默认值)
# 查询分解模型 (默认: deepseek-chat)
DEEPSEEK_MODEL_QUERY_DECOMPOSITION=deepseek-chat

# 计划生成模型 (默认: deepseek-reasoner，推理模型)
DEEPSEEK_MODEL_PLAN_GENERATION=deepseek-reasoner

# 答案生成模型 (默认: deepseek-chat)
DEEPSEEK_MODEL_ANSWER_GENERATION=deepseek-chat

# DeepSeek 超时配置 (可选，如果设置了会覆盖 LLM_TIMEOUT)
# DEEPSEEK_TIMEOUT=60.0

# DeepSeek 模型配置 (可选，不设置则使用默认值)
# 查询分解模型 (默认: deepseek-chat)
DEEPSEEK_MODEL_QUERY_DECOMPOSITION=deepseek-chat

# 计划生成模型 (默认: deepseek-reasoner，推理模型)
DEEPSEEK_MODEL_PLAN_GENERATION=deepseek-reasoner

# 答案生成模型 (默认: deepseek-chat)
DEEPSEEK_MODEL_ANSWER_GENERATION=deepseek-chat

# DeepSeek 超时配置 (可选，如果设置了会覆盖 LLM_TIMEOUT)
# DEEPSEEK_TIMEOUT=60.0

# ============================================================
# Qwen 配置 (Qwen Configuration)
# ============================================================
# Qwen API Key (可选，如果使用 Qwen 模型)
# Qwen 使用 OpenAI 兼容的 API 接口（通过阿里云 DashScope）
QWEN_API_KEY=

# Qwen Base URL (可选，默认值为 https://dashscope.aliyuncs.com/compatible-mode/v1)
QWEN_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1

# Qwen 模型配置 (可选，不设置则使用默认值)
# 查询分解模型 (默认: qwen-turbo，快速)
QWEN_MODEL_QUERY_DECOMPOSITION=qwen-turbo

# 计划生成模型 (默认: qwen-max，高质量)
QWEN_MODEL_PLAN_GENERATION=qwen-max

# 答案生成模型 (默认: qwen-plus，平衡)
QWEN_MODEL_ANSWER_GENERATION=qwen-plus

# Qwen 超时配置 (可选，如果设置了会覆盖 LLM_TIMEOUT)
# QWEN_TIMEOUT=60.0

# Qwen 模型配置 (可选，不设置则使用默认值)
# 查询分解模型 (默认: qwen-turbo，快速)
QWEN_MODEL_QUERY_DECOMPOSITION=qwen-turbo

# 计划生成模型 (默认: qwen-max，高质量)
QWEN_MODEL_PLAN_GENERATION=qwen-max

# 答案生成模型 (默认: qwen-plus，平衡)
QWEN_MODEL_ANSWER_GENERATION=qwen-plus

# Qwen 超时配置 (可选，如果设置了会覆盖 LLM_TIMEOUT)
# QWEN_TIMEOUT=60.0

# ============================================================
# Jina 配置 (Jina Configuration)
# ============================================================
# Jina AI Embedding API Key (用于生成文本嵌入向量)
# 如果不设置，向量搜索功能将不可用
JINA_API_KEY=

# Jina Base URL (可选，Jina 有默认的 base_url)
JINA_BASE_URL=

# Jina 超时配置 (可选，如果设置了会覆盖 LLM_TIMEOUT)
# 默认值：30.0 秒
JINA_TIMEOUT=30.0

# Jina 超时配置 (可选，如果设置了会覆盖 LLM_TIMEOUT)
# 默认值：30.0 秒
JINA_TIMEOUT=30.0

# Jina 模型配置 (可选，不设置则使用默认值)
# 嵌入模型 (默认: jina-embeddings-v3)
# 可选值: jina-embeddings-v3, jina-embeddings-v2 等
JINA_MODEL_EMBEDDING=jina-embeddings-v3

# ============================================================
# 向量数据库配置 (Vector Database Configuration)
# ============================================================

# 向量存储模式：local（本地文件系统，默认）/ memory（内存）/ remote（远程服务）
# 默认值：local
VECTOR_STORE_MODE=local

# 本地存储路径（仅 local 模式有效）
# 如果未设置，默认使用项目根目录下的 qdrant_data/ 文件夹
# 示例：VECTOR_STORE_PATH=../qdrant_data
# VECTOR_STORE_PATH=

# Qdrant 远程服务配置（仅 remote 模式有效）
# 方式1：使用 URL（推荐）
# QDRANT_URL=http://localhost:6333

# 方式2：使用 Host + Port（向后兼容）
QDRANT_HOST=localhost
QDRANT_PORT=6333
# QDRANT_API_KEY=  # 可选，如果 Qdrant 需要认证

# ============================================================
# 语义层配置 (Semantics Configuration)
# ============================================================
# YAML 配置文件目录路径（相对于项目根目录）
# 默认值为 "semantics"
SEMANTICS_YAML_PATH=semantics

# ============================================================
# 流水线配置 (Pipeline Configuration)
# ============================================================
# 以下配置项有默认值，可根据需要覆盖
# 配置项名称使用下划线命名（pydantic-settings 会自动转换）

# 检索配置 (Retrieval Configuration)
# VECTOR_SEARCH_TOP_K=20
# MAX_TERM_RECALL=20
# SIMILARITY_THRESHOLD=0.4

# 验证配置 (Validation Configuration)
# DEFAULT_LIMIT=100
# MAX_LIMIT_CAP=1000

# 执行配置 (Execution Configuration)
# EXECUTION_TIMEOUT_MS=5000
# MAX_RESULT_ROWS=5000

# LLM 配置 (LLM Configuration)
# MAX_LLM_ROWS=50

# ============================================================
# 日志配置 (Logging Configuration)
# ============================================================
# 日志级别: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO
